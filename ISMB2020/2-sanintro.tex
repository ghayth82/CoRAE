\section{Introduction}

A major issue while working with recent omics data is the availability of greater number of features in comparison to the available number of samples leading to a highly imbalanced feature-sample ratio. 
It may be great to argue that the larger the feature set, the better the classification that is possible. 
However, in the contrary, in a general setting, not all of these features will be necessary for optimal classification. 
Only a selected number of significant features when used with the classifier can lead to optimal classification leading to the distinguishing samples that belong to different classes. 
A large part of the remaining features are not too significant and could be either noise, irrelevant to the study or even redundant~\citep{pirgazi2019efficient}. 
The use of such insignificant features can lead to unwanted computational complexities and hamper the performance of the system. 
This is more pronounced when working with data having high dimensionality. 
Thus, it is essential to identify the set of significant features that can provide us with the optimal classification and clustering. 
For this to be accomplished, we need a robust method that can eliminate the redundant features and noise that do not have any information about the labels leaving us with only relevant features~\cite{liu2012feature}. 

Any dataset with $N$ number of features has $2^N$ possible subset of features. 
In the presence of such large number of possible combinations, finding the best subset of N features is computationally challenging and expensive~\cite{liang2018review}.
An optimally selected set of features optimizes the performance of the models and also helps in alleviating the effect of \emph{overfitting} and \emph{high-dimensionality}. 
Along with the above benefits, selecting the appropriate features helps in easier interpretation of the model and thus its predictions. 
Also, the use of the gratuitous features can significantly impact the training speeds and the accuracy of the learning models. 
Overall, appropriate feature selection can provide the following advantages: (a)~reduced cost for computation and storage, (b)~adequate use of the available sample set for improved performance, (c)~improved timing for classification and thus predictions, (d)~easier interpretation of the the data and thus the final predictions.


Filter, wrapper, embedded methods are the three general classes / types of feature selection algorithms. 
Numerous algorithms have been proposed for each of these types of feature selection methods. 
The filtering method works by a ranking the features using a statistical score that is assigned to each of them depending on their relevance to the class type. 
In both the univariate and multivariate filter methods, the interactions among features are disregarded in the selection process. 
Studies like the ones in Pearson correlation coefficient(PC), t-statistics(TS) \cite{speed2003statistical}, F-Test \cite{ding2005minimum}, and ANOVA \cite{ding2015identification} are examples where the filtering method is used. 
It is observed that these methods are effective for selecting features in high-dimensional data because of the reduced computation expenses. 
However, they fail to provide good accuracy as discussed in~\cite{sun2018cross}.

 
As an enhancement, the wrapper method is proposed with a learning algorithm and a classifier to find a suitable subset of features. 
Initially, a random solution is generated following which, an objective function is maximized using black-box optimization methods~\cite{rau2019exploring} like simulated annealing~\cite{jeong2018feature}, particle swarm optimization~\cite{xue2012particle}, genetic algorithm~\cite{wu2011feature} and, ant colony optimization~\cite{kabir2012new}. 
The evaluation of every candidate subset of the feature iteratively by the method leads to identification of a strong relationship between features however with an increase in the computational expense. 
Embedded methods are very efficient as they are a part of the learning phase. 
This helps reduce the computation costs. 
Well-known example of the embedded method are LASSO~\cite{tibshirani1996regression}, recursive feature ellimation with state vector machine estimator (SVM-RFE) \cite{abdullah2019, guyon2002gene, fang2019tightly}, random forest \cite{pouyan2018random, ram2017classification}, Adabost \cite{wang2012adaboost}, KNN \cite{le2019statistical}, and autoencoder \cite{lu2019autoencoder}.


%In general, feature selection methods are useful to get a good insight about large and complex datasets which can simplify the learning process of any machine learning algorithm. 
In general, the use of feature selection is worthwhile when using the whole set of features is difficult or the cost of execution is high. 
As an example, the gene expression dataset contains more than 60 thousand features with a very low number of samples. 
It thus is important to answer the impending questions: \textit{Is it possible to identify important genes those expressions can classify available disease or cancer type?}. 
Feature selection works differently compared to the standard dimension reduction techniques such as principal component analysis (PCA)~\cite{hotelling1933analysis}, and autoencoders~\cite{hinton2006reducing}. 
The above mentioned methods can preserve maximum variance with a highly reduced number of features. 
However, these methods do not provide the original features of the dataset making it difficult to improbable to eliminate redundant or irrelevant features from the dataset. 


In this paper, we propose a novel feature subset selection method that increases the power of a standard deep autoencoder for differentiable feature selection. 
Our major contributions in this paper is two fold: (a)~we create a new variant of the standard autoencoder by introducing the use of a concrete relaxation discrete random variable selection layer for encoding allowing selection of user-defined number of original features, (b)~evaluation of the performance of the proposed approach on coding and non-coding gene expression datasets, (c)~modifying the codebase of a standard autoencode to realize the proposed approach. Our initial evaluations show the improved performance of the proposed approach in comparison to the existing state-of-the-art methods in identifying the top 100 coding and non-coding genes that can distinguish 33 different types of cancers.
The results also show a significant increase in the classification performance up to 99\%.
Idea of concrete distribution is adapted from \cite{maddison2016concrete, kingma2013auto}, and reparameterization technique to minimize the loss and reconstruction error from \cite{abid2019concrete}.

