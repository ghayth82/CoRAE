\PassOptionsToPackage{utf8}{inputenc}
\documentclass{bioinfo}
\copyrightyear{2020} \pubyear{2020}
\newcommand{\R}{\mathbb{R}}

\usepackage{amsmath}
\usepackage{url}
\usepackage{multirow}

\DeclareMathOperator*{\argmax}{argmax}
\usepackage[ruled,vlined]{algorithm2e}

\access{Advance Access Publication Date: Day Month Year}
\appnotes{ISMB 2020}
\begin{document}
\firstpage{1}

\subtitle{Subject Section}

\title[short Title]{
CoRAE: Concreate Relaxation Autoencoder for Differentiable Gene Selection and Pan-Cancer Classification
% Pan-Cancer Feature Selection and Classification Reveals Key RNAs for Cancers
% Epigenetic Landscape Using Pan-Cancer Feature Selection and Classification
}
\author[Sample \textit{et~al}.]{Abdullah Al Mamun, Ananda Mondal*}
% $^{\text{\sfb 1,}*}$, Co-Author\,$^{\text{\sfb 2}}$ and Co-Author\,$^{\text{\sfb 2,}*}$}

\address{
% $^{\text{\sf 1}}$
School of Computing and Information Sciences \\
Florida International University, Miami, US
% $^{\text{\sf 2}}$
% Department, Institution, City, Post Code,Country.
}

\corresp{$^\ast$To whom correspondence should be addressed.}

\history{Received on XXXXX; revised on XXXXX; accepted on XXXXX}

\editor{Associate Editor: XXXXXXX}

\abstract{\textbf{Motivation:} Selecting relevant features from a high-dimensional dataset is a critical study. It aims to select a small subset of features that will increase accuracy and decrease the cost of data classification or clustering. Due to high-dimension with a low number of samples in omics data, classification models encounter over-fitting problem. Thus, there is an urgent need for efficient feature selection methods that will be capable of selecting relevant features. 
In recent years, standard autoencoder and its variations have been used to select latent features to increase the classification performance. However, these methods are unable to provide which original features are contributing to these latent features. In this paper, we introduced a novel global feature selection method based on concrete relaxation discrete random variable selection,  which can efficiently identify a subset of most significant features that have an effective contribution in data reconstruction and classification. The proposed method is a variation of standard autoencoder where a concrete feature selection layer is added in the encoder and a standard neural network is used as a decoder. 
%During training, a predefined temperature of the feature selection layer steadily decreases which allows the model to learn a user-specified number of discrete features. Also, during testing, only selected features can be used to reconstruct the input in the decoder.
\\
\textbf{Results:} We evaluated the proposed method using coding and non-coding gene expression profiles of 33 different cancers from TCGA. It significantly outperforms state-of-the-art methods in identifying top coding and non-coding genes. Later, expression values of selected genes are used to train a linear classifier to distinguish 33 cancer types where features selected by CoRAE shows highest performance in terms of five evaluation metrics: accuracy 99\%, precision 98\%, recall 98\%, f1 score 99\%, and mean squared error 2.86. 
%The proposed method can be implemented by adding a few lines of code to the standard autoencoder.
\\\\
\textbf{Availability:} Source code and an example dataset are available at https://github.com/pwaabdullah/CoRAE\\
\textbf{Contact:} \href{amondal@fiu.edu}{amondal@fiu.edu}\\
\textbf{Supplementary information:} Supplementary data are available at \textit{Bioinformatics}
online.}

\maketitle
\section{Introduction} \label{intro}
A major issue while working with recent omics data is the availability of greater number of features in comparison to the available number of samples leading to a highly imbalanced feature-sample ratio. 
It may be great to argue that the larger the feature set, the better the classification that is possible. 
However, on the contrary, in a general setting, not all of these features will be necessary for optimal classification. 
Only a selected number of significant features, when used with the classifier, can lead to optimal classification leading to distinguishing samples that belong to different classes. 
A large part of the remaining features are not too significant and could be either noise, irrelevant to the study or even redundant~\citep{pirgazi2019efficient}. 
The use of such insignificant features can lead to unwanted computational complexities and hamper the performance of the system. 
This is more pronounced when working with data having high dimensionality. 
Thus, it is essential to identify the set of significant features that can provide us with the optimal classification and clustering. 
For this to be accomplished, we need a robust method that can eliminate the redundant features and noise that do not have any information about the labels leaving us with only relevant features~\citep{liu2012feature}. 

Any dataset with $N$ number of features has $2^N$ possible subset of features ~\citep{pirgazi2019efficient}. 
In the presence of such a large number of possible combinations, finding the best subset of $N$ features is computationally challenging and expensive~\citep{liang2018review}.
An optimally selected set of features optimizes the performance of the models and also helps in alleviating the effect of \emph{overfitting} and \emph{high-dimensionality}. 
Along with the above benefits, selecting the appropriate features helps in easier interpretation of the model and thus its predictions. 
Also, the use of the gratuitous features can significantly impact the training speeds and the accuracy of the learning models. 
Overall, appropriate feature selection can provide the following advantages: (a)~reduced cost for computation and storage, (b)~adequate use of the available sample set for improved performance, (c)~improved timing for classification and thus predictions, (d)~easier interpretation of the data and thus the final predictions.


Filter, wrapper, embedded methods are the three general classes/types of feature selection techniques. 
Numerous algorithms have been proposed for each of these types of feature selection methods. 
The filtering method works by ranking the features using a statistical score that is assigned to each of them depending on their relevance to the class type. 
In both univariate and multivariate filter methods, the interactions among features are disregarded in the selection process. 
Studies like the ones in Pearson correlation coefficient(PCC), t-statistics(TS) \citep{speed2003statistical}, F-Test \citep{ding2005minimum}, and ANOVA \citep{ding2015identification} are examples where the filtering method is used. 
It is observed that these methods are effective for selecting features in high-dimensional data because of the reduced computation expenses. 
However, they fail to provide good accuracy as discussed in~\citep{sun2018cross}.

 
As an enhancement, the wrapper method is proposed with a learning algorithm and a classifier to find a suitable subset of features. 
Initially, a random solution is generated following which, an objective function is maximized using black-box optimization methods~\citep{rau2019exploring} like simulated annealing~\citep{jeong2018feature}, particle swarm optimization~\citep{xue2012particle}, genetic algorithm~\citep{wu2011feature} and, ant colony optimization~\citep{kabir2012new}. 
The iterative evaluation of every candidate subset of the feature by the method leads to the identification of a strong relationship between features,  however with an increase in the computational expense. 
Embedded methods are very efficient as they are a part of the learning phase. 
This helps reduce computation costs. 
Well-known example of the embedded method are LASSO~\citep{tibshirani1996regression}, recursive feature ellimation with state vector machine estimator (SVM-RFE) \citep{abdullah2019, guyon2002gene, fang2019tightly}, random forest \citep{pouyan2018random, ram2017classification}, Adabost \citep{wang2012adaboost}, KNN \citep{le2019statistical}, and autoencoder \citep{lu2019autoencoder}.


%In general, feature selection methods are useful to get a good insight about large and complex datasets which can simplify the learning process of any machine learning algorithm. 
In general, the use of feature selection is worthwhile when using the whole set of features is difficult or the cost of execution is high. 
As an example, the gene expression dataset contains more than 60 thousand features with a very low number of samples. 
Consequently, it is important to answer the impending questions: \textit{Is it possible to identify important genes whose expressions can classify disease or cancer types?}. 
Feature selection works differently compared to the standard dimension reduction techniques such as principal component analysis (PCA)~\citep{hotelling1933analysis}, and autoencoders~\citep{hinton2006reducing}. 
The above-mentioned methods can preserve maximum variance with a highly reduced number of features. 
However, these methods do not provide the original features of the dataset making it difficult to eliminate redundant or irrelevant features from the dataset. 


In this paper, we propose a novel feature subset selection method that increases the power of a standard deep autoencoder for differentiable feature selection. 
Our major contributions in this paper are two-fold: (a)~we created a new variant of the standard autoencoder by introducing the use of a concrete relaxation discrete random variable selection layer for encoding that allows selection of user-defined number of original features, (b)~evaluation of the performance of the proposed approach in classifying 33 cancers by selecting features from cancer patients only. (c)~modifying the codebase of a standard autoencoder to realize the proposed approach. Our initial evaluations show the improved performance of the proposed approach in comparison to the existing state-of-the-art methods in identifying the top 100 coding and non-coding genes that can distinguish 33 different types of cancers.
The results also show a significant increase in the classification performance up to 99\% as shown in Table~\ref{Tab:02}.
Idea of concrete distribution is adapted from \citep{maddison2016concrete, kingma2013auto}, and reparameterization technique to minimize the loss and reconstruction error from \citep{abid2019concrete}.
\begin{table*}[hbt]
\processtable{ \textbf{Sample distribution for 33 cancers along with 75-25 split for training and testing.}  \label{Tab:01}} {\begin{tabular}{@{}llllll|llllll@{}}\toprule Sl	&	Cancer site name	&	Acronym	&	\#Sample	&	\#Train	&	\#Test	&	Sl	&	Cancer site name	&	Acronym	&	\#Sample	&	\#Train	&	\#Test\\\midrule

1	&	Adrenocortical Cancer	&	ACC	&	77	&	57	&	20	&	18	&	Lung Squamous Cell Carcinoma	&	LUSC	&	498	&	373	&	125	\\
2	&	Bladder Cancer	&	BLCA	&	407	&	305	&	102	&	19	&	Mesothelioma	&	MESO	&	86	&	64	&	22	\\
3	&	Breast Cancer	&	BRCA	&	1089	&	816	&	273	&	20	&	Ovarian Cancer	&	OV	&	375	&	281	&	94	\\
4	&	Cervical Cancer	&	CESC	&	304	&	228	&	76	&	21	&	Pancreatic Cancer	&	PAAD	&	177	&	132	&	45	\\
5	&	Bile Duct Cancer	&	CHOL	&	36	&	27	&	9	&	22	&	Pheochromocytoma \& Paraganglioma	&	PCPG	&	177	&	132	&	45	\\
6	&	Colon Cancer	&	COAD	&	301	&	225	&	76	&	23	&	Prostate Cancer	&	PRAD	&	493	&	369	&	124	\\
7	&	Large B-cell Lymphoma	&	DLBC	&	47	&	35	&	12	&	24	&	Rectal Cancer	&	READ	&	95	&	71	&	24	\\
8	&	Esophageal Cancer	&	ESCA	&	161	&	120	&	41	&	25	&	Sarcoma	&	SARC	&	258	&	193	&	65	\\
9	&	Glioblastoma	&	GBM	&	158	&	118	&	40	&	26	&	Melanoma	&	SKCM	&	465	&	348	&	117	\\
10	&	Head and Neck Cancer	&	HNSC	&	499	&	374	&	125	&	27	&	Stomach Cancer	&	STAD	&	378	&	283	&	95	\\
11	&	Kidney Chromophobe	&	KICH	&	66	&	49	&	17	&	28	&	Testicular Cancer	&	TGCT	&	132	&	99	&	33	\\
12	&	Kidney Clear Cell Carcinoma	&	KIRC	&	527	&	395	&	132	&	29	&	Thyroid Cancer	&	THCA	&	501	&	375	&	126	\\
13	&	Kidney Papillary Cell Carcinoma	&	KIRP	&	287	&	215	&	72	&	30	&	Thymoma	&	THYM	&	118	&	88	&	30	\\
14	&	Acute Myeloid Leukemia	&	LAML	&	147	&	110	&	37	&	31	&	Endometrioid Cancer	&	UCEC	&	184	&	138	&	46	\\
15	&	Lower Grade Glioma	&	LGG	&	507	&	380	&	127	&	32	&	Uterine Carcinosarcoma	&	UCS	&	56	&	42	&	14	\\
16	&	Liver Cancer	&	LIHC	&	369	&	276	&	93	&	33	&	Ocular melanomas	&	UVM	&	79	&	59	&	20	\\
17	&	Lung Adenocarcinoma	&	LUAD	&	512	&	384	&	128	&		&	Total	&		&	9566	&	7161	&	2405\\\botrule
\end{tabular}}{}
\end{table*}
\section{Materials and Methods}
\subsection{Coding and Non-coding Gene Expression}
To validate the proposed idea, TCGA RNAseq cancer expression profiles and clinical data for $33$ cancers ($n$=9566 cancer patients) were downloaded from UCSC Xena database (https://xenabrowser.net). 
%TCGA processed raw RNAseq data uses Illumina HiSeq 2000 RNA sequencing platform where per-gene normalized abundance estimation is calculated using the FPKM method. RNASeq normalized counts are then log transformed after adding a constant of $1$. Subsequently, UCSC re-processed this using GENCODE v23 transcript annotation to quantify protein coding and non-coding transcripts expression \citep{harrow2006gencode}. Coding genes refer to mRNA whereas non-coding genes refer to long non-coding RNA (lncRNA) in this experiment. To improve the focus on individual feature selection, we separated the mRNA and lncRNA expression from the combined database using the standard list of lncRNAs provided by TANRIC \citep{li2015tanric}. Another important reason for performing the experiments on individual RNA types is because of their varying expression levels.  The number of mRNA and lncRNA are 18731, 12309 respectively. We merged all the cancer samples for individual RNA types to aid further experimentation. Each row is mapped to a unique Ensemble ID, and each column mapped to a patient ID. Normal patients or RNA with missing data were removed from the original dataset. 
This dataset contains expression profiles of around $60k$ RNA including coding and non-coding (miRNA, lncRNA, etc). In this study, expression profiles of mRNA and lncRNA are considered for model evaluations. The number of mRNA and lncRNA are 18,731 and 12,309 respectively. Due to their varying expression levels, a separate experiment was conducted for mRNA and lncRNA. This study was based on cancer patients only. So normal samples available in the same cancer are removed. The final dataset contains 9566 cancer patients as shown in Table~\ref{Tab:01}. Each RNA expression was further processed using a min-max normalization method to achieve good training performance. 
\begin{figure*}[h!]
    \centering
    \includegraphics[scale=0.5]{fig/architecture.pdf}
    \caption{\textbf{Architecture of Concrete Relaxation Autoencoder.} Proposed feature selection architecture consists of an encoder and a decoder. The layer after input layer in encoder is called concrete feature selection layer shown in yellow. This layer has $k$ number of node where each node is for each feature to be selected. During the training stage, the $i^{th}$ node $v^{(i)}$ takes the value $x^Tf^{(i)}$.
During testing stage, these weights are fixed and the element with the highest value is selected by the corresponding $i^{th}$ hidden node.
The architecture of the decoder remains the same during train and test stages.}
    \label{fig:architecture}
\end{figure*}
\subsection{Concrete Relaxation Autoencoder} \label{CoRAE}
The concrete relaxation autoencoder (CoRAE) is a variation of the original autoencoder (AE) \citep{hinton2006reducing} which is used for dimension reduction. 
It is a neural network that consists of two parts: (a)~an encoder that selects latent features and (b)~a decoder that uses selected features to reconstruct an output that matches the input. 
Instead of using a sequence of fully connected layers in the encoder,  we propose a concrete relaxation based feature selection layer where the user can define the number of nodes (features), $k$ as shown in Figure~\ref{fig:architecture}. 
This layer selects a probabilistic linear arrangement of input features while training, which converges to a discrete set of $k$ features by the end of training and during the testing. 

The original features are selected based on the temperature of this layer which is tuned using an \emph{annealing schedule}. More specifically, the concrete selector layer identifies $k$ important features as the temperature decreases to zero. 
For reconstructing the input, a simple decoder similar to the ones associated with a standard AE is used. 
This simple neural network can be updated based on the characteristics of the data and its complexity.
% ------read

The layer that selects the features and highlighted in Figure \ref{fig:architecture} is called concrete variable selctor layer adopted from the concrete distribution~\citep{maddison2016concrete} and categorical representation \citep{jang2016categorical}. 
Since backpropagation does not allow computation of the parameters' gradient through stochastic nodes of a standard autoencoder, Gumbel $softmax$ distribution $g$~\citep{gumbel1954statistical} is the right choice to pick samples $f$ from categorical distribution with class probabilities $\alpha_k$. 
\begin{equation}
	f = \textbf{one-hot} \, (\argmax_k \, [g_k + log \, \alpha_k])
\end{equation}
Because $\argmax$ is not differentiable, a simple $softmax$ function can be used as a continuous approximation of $\argmax$. The aim of using Concrete random variables is to relax the state of a discrete variable and the relaxation degree is controlled by a temperature parameter $\tau \in (0, \infty)$. To sample a concrete random variable in $z$ dimension with parameter $\alpha \in \R ^z>0$ and $\tau$, one must sample a $z$-dimensional vector of $i.i.d.$ (independent and identically distributed) samples from a Gumbel distribution, $g.$ Then each element of the sample $f$ from the Concrete distribution can be defined as:
\begin{equation}
    f_k = \frac{exp((log \alpha_k + g_k)/\tau)}{\sum_{i=1} ^z exp((log \alpha_i + g_i)/T) } \; for \; k = 1,.....,z
\end{equation}
where $f_k$ refers to the $k_{th}$ element in a particular sample vector. 
With the limit $\tau \to 0$, the concrete variable uniformly progresses the discrete distribution, producing one-hot vector with $f_k = 1$ with a probabilistic chance of $\alpha _k/\sum_p \alpha _p$.
The advantage of using a concrete random discrete variable is that it is differentiable $w.r.t$ $\alpha$ using the reparameterization technique as mentioned by \citep{kingma2013auto}.

More concisely, the way original feature is selected using the concrete random variable as follows: a $z$-dimensional concrete random variable $f^{(i)}$ is sampled for each node of the selector layer with $k$ nodes where $i$ refers to the index of the node, $i \in \{1...k\}$. The output of the $i^{th}$ node is $ \textbf{x}.f^{(i)}$. Although it is a combination of the input feature's weight, every node of the selector layer produces exactly one of the original input features in the limit $\tau \to 0$. After training the network, a discrete $\argmax$ layer is replaced with the concrete selector layer by which $x_{\argmax_j \; \alpha_j^{(i)}}$ is produced as an output of $i^{th}$ node during the testing phase. The value of $\alpha_i$ initially starts with a small positive random number so that it can explore various combinations of input features. As the model is being trained, the value of $\alpha_i$, in other words, the probability of class $i$ becomes more stable. As a result, the model reduces its stochasticity rather increases the confidence in drawing a particular subset of features. 

The temperature of the random variable in the selector layer has a significant impact in forming the output of each node. Initially, when $\tau$ is high, search space is large since it considers a linear combination of all features. In contrast, the selector layer will not be able to search all possible combinations of features in low $\tau$ and thus, model converges to a bad local minimum. Effect of changing the temperature in feature selection is shown in Figure~\ref{fig:temp-change}. Instead of using a fixed temperature, a simple annealing scheduling scheme is used for every concrete variable. It starts with an user-defined high temperature ($\tau_s$) and steadily lowers the temperature until it touches the ending bound ($\tau_e$) by every epoch as follows: 
\begin{equation}
\tau_{(e)} = \tau_s(\tau_N/\tau_s)^{e/N}
\end{equation}
where $T_{(e)}$ is the temperature at epoch $e$, $N$ refers to the total number of epochs. The proposed annealing schedule is good enough to explore the feature combinations during the training phase and finally lowered temperature enables the model to strict to the best set of features which is shown in Figure \ref{fig:temp}. The pseudocode of training a CoRAE is shown in Algorithm \ref{algo}.
\begin{algorithm}[h]
\SetAlgoLined
\textbf{Input:} $k$-number of feature to be selected,  $X \in \R^{n \times z}$-Input, $d$-decoder, $\theta$-number of learning rate, $N$-number of epoch, $\tau_s$-starting temperature, and $\tau_f$-stopping temperature \\
\KwResult{ $k$ number of features}
 \For{$i \; \in \{1 \; to \; z\}$}{
    Assign a positive value to each $\alpha^{(i)}$
 }
 \For{$ e \; \in \{1 \; to \; N\}$}{
    Let $\tau_e$ = $\tau_s(\tau_N/\tau_s)^e/N$\\
     \For{$ k \; \in \{1 \; to \; z\}$}{
        sample $f^{(k)} \textasciitilde \, Concrete \; (\alpha^{(k)}, \tau)$\\
        let $X^{(k)}$ = $X.f^{(k)}$, where  {$f^{(k)}$ =  argmax($\alpha^{(k)}$)}\\ 
 }
}
 \caption{Concrete relaxation autoencoder}
 \label{algo}
\end{algorithm}
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{fig/temp-change.pdf}
    \caption{\textbf{Effect of temperature in reducing search space.} For example, at starting temperature $\tau_s$, the number of input features is 10 and the number of features to be selected $k$ is 3, at the next epoch when the temperature is $\tau_1$, the number of possible features reduces to 6. After some epochs, when the temperature reaches to its lower bound $\tau_{stop}$, the number of features further reduces to 3 which is equal to the $k$}
    \label{fig:temp-change}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.45]{fig/temp-epoch-mRNA.pdf}
    \caption{\textbf{Annealing schedules for the CoRAE.} Effect of different annealing schedules on a concrete autoencoder trained on the mRNA dataset with $k$ = 100 selected features. If the temperature is exponentially decayed (the annealing schedule), the feature selected layer (model) converges to informative features.}
    \label{fig:temp}
\end{figure}
\begin{figure*}[hbt]
    \centering
    \includegraphics[scale=0.5]{fig/acc-mse.pdf}
    \caption{\textbf{Classification performance using selected RNA features.}  Comparison of CoRAE with other feature selection methods. Throughout the all values of $k$ tested on both mRNA(a) and lncRNA(c) CoRAE have highest classification accuracy. Similarly, it shows lowest reconstruction mean squared error on both mRNA(b) and lncRNA(d)}
    \label{fig:acc-mse}
  \end{figure*}
\subsection{Gene Selection, Classification, Reconstruction, and Evaluation} \label{method-details}
%\subsubsection{Feature Selection}
The encoder of the Concrete relaxation autoencoder (CoRAE) architecture is constructed with a hidden layer of $k$ nodes where $k$ is the number of genes selected. The decoder, on the other hand, is consisting of one hidden layer with $3k/2$ nodes. 
The number of nodes in this layer is tuned in a range of $[4k/7, 2k/5, 3k/2]$. 
Adam optimizer with a learning rate of $0.001$ is used for all the experiments. 
The starting temperature of the CoRAE was set to $10$ and it ends at $0.01$. 
To avoid overfitting, the dataset is split into the train and test set according to $75/25$ ratio. 
The training set is used to estimate the learning parameters and the test set is used for performance evaluation. 
To control the performance, the model is trained for the same number of epoch $100$.

 
Performance of CoRAE has been compared with state-of-the-art feature selection techniques such as LASSO and SVM-RFE on both mRNA and lncRNA expression datasets. 
In LASSO, a regularization parameter $\alpha$ decides the number of most important features. 
More precisely, the higher the $\alpha$, the more feature's coefficient shrinks to zero, and fewer features will be selected. Recursive feature elimination is a recursive method in which less important features are eliminated in every iteration. 
In the recursive feature elimination technique, SVM is used as an estimator. 
A linear kernel with a regularization parameter $C=0.05$ is used. $C$ controls the tradeoff between the error and norm of the learning weights. 
GridSearch algorithm is used to estimate the best set of parameters for SVM. 
In every iteration of RFE, the number of dropped features is set to 100. 

We extract a subset of features by varying $k$ from 10 to 500. For the comparison to be fair and along the same grounds with CoRAE, the same number of genes are selected using LASSO and SVM-RFE. 
The dataset with reduced number of features (expression of selected genes) is passed to the SVM for classifying 33 cancer types. Similarly, to reconstruct all the input features, we trained a linear regressor with no regularization and measure the reconstruction mean square error. 
LASSO and SVM-RFE are developed using the scikit-learn framework~\citep{scikit-learn} whereas CoRAE is built using Google-developed Tensorflow \citep{tensorflow2015-whitepaper} based deep learning framework Keras~\citep{chollet2015keras}. Experiments are parallelized on NVIDIA Quadro K620 GPU with 384 cores and 2GB memory devices. Five different evaluation metrics have been used to record the classification and reconstruction performance such as accuracy, precision, recall, f1 score, and mean squared error (MSE). 

Accuracy is the number of correct predictions made by the model over all kinds of predictions made. 
True positives(TP) and True Negatives(TN) are the correct predictions. 

    \begin{equation}
        Accuracy = \frac{TP+TN}{TP+TN+FP+FN}
    \end{equation}
    Precision is the number of correct positive results divided by the number of positive results predicted by the classifier. It indicates the predicted positive portion of the samples. 
    \begin{equation}
        Precision = \frac{TP}{TP+FP}
    \end{equation}
    Recall is the number of correct positive results divided by the number of all relevant samples.
    \begin{equation}
        Recall = \frac{TP}{TP+FN}
    \end{equation}
	F1 score is a measure of a test's accuracy. It considers both the precision and the recall of the test to compute the score.
	\begin{equation}
        F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}
    \end{equation}
    Mean squared error (MSE) is the average of $(\frac{1}{n} \sum _{i=1} ^n)$ of the square of the errors $(Y_i - Y'_i)$ where $Y_i$ is a true label and $Y'_i$ is a predicted label. All performance metrics are measured on the predicted labels and true labels of independent test samples. 

Top genes can be selected based on two criteria~(a) classification accuracy needs to be higher,
and~(b) the number of genes should be as less as possible so that biologists can conduct a wet lab experiment
easily. The capabilities of selected genes in pan-cancer classification is visually validated using unsupervised visualization technique t-SNE \citep{maaten2008visualizing}.
%Survival analysis is conducted for testing the prognostic capabilities of selected gene set. 
\begin{table*}[hbt]
\processtable{ \textbf{Classification and reconstruction performances for different number of selected mRNAs and lncRNAs. Here, $k$ is refere to the number of feature to be selected. Perforamnce evaluations for other $k$ are shown in \textit{supplementary-1} Table 2}\label{Tab:02}} 
{\begin{tabular}{@{}ll|lllll|llllll@{}}
\toprule
& & & &mRNA&&& & &lncRNA&&& \\
\midrule
\#Feature selected, $k$ & Method Name	&	Accuracy	&	Precision	&	Recall	&	F1	& MSE &	Accuracy	&	Precision	&	Recall	&	F1	& MSE	\\
\midrule
\multirow{3}{*}{10} &	LASSO	&	0.55	&	0.51	&	0.52	&	0.51	&	66.02	&	0.55	&	0.51	&	0.5	&	0.52	&	81.56	\\
&	SVM-RFE	&	0.63	&	0.6	&	0.53	&	0.53	&	64.69	&	0.5	&	0.4	&	0.36	&	0.35	&	88.19	\\
&	CoRAE	&	0.79	&	0.73	&	0.67	&	0.68	&	36.07	&	0.7	&	0.6	&	0.58	&	0.57	&	60.11	\\
\midrule


\multirow{3}{*}{100} &	LASSO	&	0.93	&	0.89	&	0.89	&	0.89	&	10.4	&	0.92	&	0.9	&	0.9	&	0.89	&	12.83	\\
&	SVM-RFE	&	0.92	&	0.88	&	0.88	&	0.88	&	15.04	&	0.91	&	0.87	&	0.87	&	0.87	&	14.36	\\
&	CoRAE	&	0.96	&	0.95	&	0.95	&	0.95	&	6.4	&	0.94	&	0.93	&	0.91	&	0.92	&	11.37	\\

\midrule

\multirow{3}{*}{500} &	LASSO	&	0.96	&	0.94	&	0.94	&	0.94	&	6.79	&	0.94	&	0.92	&	0.92	&	0.92	&	11.19	\\
&	SVM-RFE	&	0.95	&	0.93	&	0.93	&	0.93	&	9.47	&	0.94	&	0.92	&	0.91	&	0.91	&	10.01	\\
&	CoRAE	&	\textbf{0.99}	&	\textbf{0.98}	&	\textbf{0.98	}&	\textbf{0.98}	&	\textbf{2.86}	&	0.96	&	0.94	&	0.93	&	0.94	&	7.7	\\
\botrule
\end{tabular}}{}
\end{table*}
\section{Results}
A series of experiments were conducted to compare the performance of CoRAE with other state-of-the-art feature selection methods such as LASSO and SVM-RFE.
Each of these three methods was used to select features in the range of $10$ to $500$.
These features are then used to train a linear classifier SVM to classify 33 cancer types from expression profiles of coding and non-coding RNA separately. 
Table~\ref{Tab:02} and Figure~\ref{fig:acc-mse} show classification performance of using selected features. It can be observed that the scale used in the x-axis to depict the number of features does not increase with a consistent step size.
The initial stages of the experiments were performed with a smaller subset of the selected features as we wanted to understand the performance of the models being compared.
The optimal classification performance was observed about $100$ features were used.
Beyond this, the trend continues as shown in the figure. We continued to monitor the performance of the models until $500$ features to check for the best possible subset of features. 

For all selected $k$ number of features, as depicted in Figure~\ref{fig:acc-mse}, CoRAE has the highest accuracy and lowest error for both mRNA and lncRNA expression.
It can be observed that even with a smaller number of significant features (say $10$), the accuracy of CoRAE is close to 80\% (mRNA) and 70\% (lncRNA) whereas LASSO and SVM-RFE shows poor results for the same number of features.
The trend remains the same with the increase of number of features. With only $50$ features, accuracy of CoRAE is more than 90\%.

The CoRAE method is resilient to errors that occur during reconstruction using a small feature set. In comparison, this error is more pronounced in the other competing methods.
In case of mRNA, CoRAE starts with an MSE of $38$ and quickly reduces to a value of less than $10$ within the use of the top $100$ features as shown in Figure~\ref{fig:acc-mse}. 
The behavior in classification is highly comparable in both coding and non-coding genes.
However, coding gene expressions perform slightly better than non-coding gene expression as shown in Figure~\ref{fig:acc-mRNA-lncRNA}.
  \begin{figure}[hbt]
    \centering
    \includegraphics[scale=0.45]{fig/acc-mRNA-lncRNA.pdf}
    \caption{\textbf{Classification accuracy comparison between coding and non-coding genes expression.} Across all the $k$, coding gene expressions show slightly better classification accuracy over non-coding gene expressions. Here, these features has been seelcted using proposed method only.}
    \label{fig:acc-mRNA-lncRNA}
\end{figure}
\subsection{Selected Features Interpretation} \label{inter}
With the use of CoRAE, we are able to identify important features while allowing the user to examine the relevance of each feature by observing the corresponding estimated concrete parameter $\alpha^{(i)}$.
In CoRAE, feature selection is based on the value of vector $\alpha$ which gives the user the value of the importance score which in turn gives the power to identify features and based on their correlation with the other selected features. 
Figure \ref{fig:tsne}, highlights how the top $100$ mRNA or lncRNA, selected using the CoRAE, is capable of distinguishing $33$ cancer types. 
%It can also be observed that the features selected using the CoRAE method carries more information than the ones selected by the other methods.
%The use of such features improves prediction accuracy.
%The improved performance is highlighted in Figures~\ref{fig:acc-mse, fig:acc-mRNA-lncRNA, fig:tsne} with the CoRAE method succeeding in picking more significant features in comparison to the other state-of-the-art methods.
\begin{figure*}[hbt]
  \centering
  \includegraphics[scale=0.45]{fig/tSNE.pdf}
    \caption{\textbf{Visualization of 33 different cancer types based on top-100 CoRAE features.} Here, we show the t-SNE representation of 33 cancer samples using selected features. Each dot represents a cancer sample and each color represents a cancer type (a) t-SNE using top 100 mRNA, (b) t-SNE using top 100 lncRNA}
    \label{fig:tsne}
\end{figure*}
\section{Discussion}
In this paper, a new differentiable feature selection method called concrete relaxation autoencoder (CoRAE) is developed.
It uses re-parameterization and concrete random variable technique to allow gradients to pass through a layer that stochastically selects discrete original input features of higher significance.
The randomness of the proposed method enables it to effectively search and converge to a user-defined number of original features that maximize the objective function and minimize the loss as discussed in section \ref{CoRAE}.
The estimated parameters learned by the models can be further examined by the biologists and other stakeholders to interpret biological relevance as discussed in section \ref{inter}.
The above-mentioned characteristics of CoRAE provide it with a distinction from numerous other competing approaches which are based on regularization. 

Since CoRAE is built on top of a standard autoencoder architecture, it is easily scalable to a higher number of samples or dimensions.
It is observed that the features selected by the CoRAE outperformed the ones from the competing methods.
This paper accounts for a generalized approach of CoRAE. However, there are other avenues that can be explored using the proposed method.
As an example, CoRAE can be used to extract important genes during the molecular subtype classification of a single cancer dataset, unlike the existing approaches which are based on the multiple cancer classification.
The proposed method also provides the privilege to users to integrate multi-omics data such as gene, protein, RNAseq expression, DNA methylation, copy number and so on. 

CoRAE is easy to use and requires modifying a few lines for implementing it in the popular machine learning frameworks.
Moreover, the runtime and space complexity is similar to that of the standard autoencoder.
In addition, it allows parallelization and enhances hardware acceleration which are obvious demands for deep learning techniques.
Starting and ending temperature are the only added hyperparameters used for annealing schedule.
The default values used in the experiment are carefully identified and is found to work adequately for the various datasets.
\section{Conclusion}
In this paper, we propose a novel feature subset selection method that increases the power of a standard deep autoencoder for differentiable feature selection. 
The proposed CoRAE is a new variant of the standard autoencoder which uses a concrete relaxation discrete random variable selection layer for encoding which allows selection of a user-defined number of original features
by modifying the codebase of a standard autoencoder. We evaluate the performance of the proposed approach on coding and non-coding gene expression datasets and compare the results with state-of-the-art methods like LASSO and SVM-RFE.
Our experiments show that on publicly available gene expression cancer datasets, CoRAE efficiently maximizes the classification accuracy and minimizes the reconstruction error using a selected subset of genes.
For both mRNA and lncRNA gene expression datasets, CoRAE outperformed several sophisticated feature selection techniques.
Use of a single hidden layer in the decoder, minimizes the reconstruction error and allows for selection of features from large datasets.

As a part of the future work, we will conduct more biological validations such as survival analysis of 33 cancer patients using selected features to measure the prognostic capabilities.
Similarly, pathway analysis of selected coding and non-coding genes will be analyzed in the future.
\section*{Acknowledgements}
We are grateful to Sanjeev Kaushik Ramani for helpful contribution in writing.
% \section*{Funding}
This research is partially funded by the US National Science Foundation CAREER award \#1651917 (transferred to \#1901628) to AMM.

\bibliographystyle{natbib}
%\bibliographystyle{achemnat}
%\bibliographystyle{plainnat}
%\bibliographystyle{abbrv}
%\bibliographystyle{bioinformatics}
\bibliography{document}

\end{document}