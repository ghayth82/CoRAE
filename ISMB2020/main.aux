\relax 
\citation{pirgazi2019efficient}
\citation{liu2012feature}
\citation{liang2018review}
\citation{speed2003statistical}
\citation{ding2005minimum}
\citation{ding2015identification}
\citation{sun2018cross}
\citation{rau2019exploring}
\citation{jeong2018feature}
\citation{xue2012particle}
\citation{wu2011feature}
\citation{kabir2012new}
\citation{tibshirani1996regression}
\citation{abdullah2019}
\citation{guyon2002gene}
\citation{fang2019tightly}
\citation{pouyan2018random}
\citation{ram2017classification}
\citation{wang2012adaboost}
\citation{le2019statistical}
\citation{lu2019autoencoder}
\citation{hotelling1933analysis}
\citation{hinton2006reducing}
\citation{maddison2016concrete}
\citation{kingma2013auto}
\citation{abid2019concrete}
\newlabel{^_1}{{}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\newlabel{intro}{{1}{1}}
\citation{harrow2006gencode}
\citation{li2015tanric}
\citation{hinton2006reducing}
\citation{maddison2016concrete}
\citation{jang2016categorical}
\citation{gumbel1954statistical}
\citation{kingma2013auto}
\@writefile{toc}{\contentsline {section}{\numberline {2}Materials and Methods}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{2.1}}Coding and Non-coding Gene Expression}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{2.2}}Concrete Relaxation Autoencoder}{2}}
\newlabel{CoRAE}{{{2.2}}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{2.3}}Gene Selection, Classification, Reconstruction, and Evaluation}{2}}
\newlabel{method-details}{{{2.3}}{2}}
\citation{scikit-learn}
\citation{tensorflow2015-whitepaper}
\citation{chollet2015keras}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  \textbf  {Sample Distribution for 33 cancers along with 75-25 split for training and testing.} }}{3}}
\newlabel{Tab:01}{{1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Architecture of Concrete Relaxation Autoencoder.} Proposed feature selection architecture consists of an encoder and a decoder. The layer after input layer in encoder is called concrete feature selection layer shown in yellow. This layer has $k$ number of node where each node is for each feature to be selected. During the training stage, the $i^{th}$ node $v^{(i)}$ takes the value $x^Tf^{(i)}$. During testing stage, these weights are fixed and the element with the highest value is selected by the corresponding $i^{th}$ hidde n node. The architecture of the decoder remains the same during train and test stage.}}{3}}
\newlabel{fig:architecture}{{1}{3}}
\citation{maaten2008visualizing}
\bibstyle{bioinformatics}
\bibdata{document}
\bibcite{tensorflow2015-whitepaper}{{1}{2015}{{Abadi et~al.}}{{Abadi, Agarwal, Barham, Brevdo, Chen, Citro, Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia, Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah, Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan, Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and Zheng}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{3.1}}Interpreting Related Features}{4}}
\newlabel{inter}{{{3.1}}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{4}}
\bibcite{abdullah2019}{{2}{2019}{{Abdullah}}{{}}}
\bibcite{abid2019concrete}{{3}{2019}{{Abid et~al.}}{{Abid, Balin, and Zou}}}
\bibcite{chollet2015keras}{{4}{2015}{{Chollet et~al.}}{{}}}
\bibcite{ding2005minimum}{{5}{2005}{{Ding and Peng}}{{}}}
\bibcite{ding2015identification}{{6}{2015}{{Ding and Li}}{{}}}
\bibcite{fang2019tightly}{{7}{2019}{{Fang}}{{}}}
\bibcite{gumbel1954statistical}{{8}{1954}{{Gumbel}}{{}}}
\bibcite{guyon2002gene}{{9}{2002}{{Guyon et~al.}}{{Guyon, Weston, Barnhill, and Vapnik}}}
\bibcite{harrow2006gencode}{{10}{2006}{{Harrow et~al.}}{{Harrow, Denoeud, Frankish, Reymond, Chen, Chrast, Lagarde, Gilbert, Storey, Swarbreck, et~al.}}}
\bibcite{hinton2006reducing}{{11}{2006}{{Hinton and Salakhutdinov}}{{}}}
\bibcite{hotelling1933analysis}{{12}{1933}{{Hotelling}}{{}}}
\bibcite{jang2016categorical}{{13}{2016}{{Jang et~al.}}{{Jang, Gu, and Poole}}}
\bibcite{jeong2018feature}{{14}{2018}{{Jeong et~al.}}{{Jeong, Kim, Kim, Lee, Kim, and Kang}}}
\bibcite{kabir2012new}{{15}{2012}{{Kabir et~al.}}{{Kabir, Shahjahan, and Murase}}}
\bibcite{kingma2013auto}{{16}{2013}{{Kingma and Welling}}{{}}}
\bibcite{le2019statistical}{{17}{2019}{{Le et~al.}}{{Le, Urbanowicz, Moore, and McKinney}}}
\bibcite{li2015tanric}{{18}{2015}{{Li et~al.}}{{Li, Han, Roebuck, Diao, Liu, Yuan, Weinstein, and Liang}}}
\bibcite{liang2018review}{{19}{2018}{{Liang et~al.}}{{Liang, Ma, Yang, Wang, and Ma}}}
\bibcite{liu2012feature}{{20}{2012}{{Liu and Motoda}}{{}}}
\bibcite{lu2019autoencoder}{{21}{2019}{{Lu et~al.}}{{Lu, Gu, Wang, Wang, and Qin}}}
\bibcite{maaten2008visualizing}{{22}{2008}{{Maaten and Hinton}}{{}}}
\bibcite{maddison2016concrete}{{23}{2016}{{Maddison et~al.}}{{Maddison, Mnih, and Teh}}}
\bibcite{scikit-learn}{{24}{2011}{{Pedregosa et~al.}}{{Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, and Duchesnay}}}
\bibcite{pirgazi2019efficient}{{25}{2019}{{Pirgazi et~al.}}{{Pirgazi, Alimoradi, Abharian, and Olyaee}}}
\bibcite{pouyan2018random}{{26}{2018}{{Pouyan and Kostka}}{{}}}
\bibcite{ram2017classification}{{27}{2017}{{Ram et~al.}}{{Ram, Najafi, and Shakeri}}}
\bibcite{rau2019exploring}{{28}{2019}{{Rau et~al.}}{{Rau, Flister, Rui, and Auer}}}
\bibcite{speed2003statistical}{{29}{2003}{{Speed}}{{}}}
\bibcite{sun2018cross}{{30}{2018}{{Sun et~al.}}{{Sun, Lu, and Li}}}
\bibcite{tibshirani1996regression}{{31}{1996}{{Tibshirani}}{{}}}
\bibcite{wang2012adaboost}{{32}{2012}{{Wang}}{{}}}
\bibcite{wu2011feature}{{33}{2011}{{Wu et~al.}}{{Wu, Tang, Hor, and Wu}}}
\bibcite{xue2012particle}{{34}{2012}{{Xue et~al.}}{{Xue, Zhang, and Browne}}}
\global\@namedef{@lastpage@}{5}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Temperature Effect.} }}{6}}
\newlabel{fig:temp-change}{{2}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Annealing schedules for the CoRAE.} Effect of different annealing schedules on a concrete autoencoder trained on the mRNA dataset with $k$ = 100 selected features. If the temperature is exponentially decayed (the annealing schedule), the feature selected layer (model) converges to informative features.}}{6}}
\newlabel{fig:temp}{{3}{6}}
