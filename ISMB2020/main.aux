\relax 
\citation{pirgazi2019efficient}
\citation{liu2012feature}
\citation{pirgazi2019efficient}
\citation{liang2018review}
\newlabel{^_1}{{}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\newlabel{intro}{{1}{1}}
\citation{speed2003statistical}
\citation{ding2005minimum}
\citation{ding2015identification}
\citation{sun2018cross}
\citation{rau2019exploring}
\citation{jeong2018feature}
\citation{xue2012particle}
\citation{wu2011feature}
\citation{kabir2012new}
\citation{tibshirani1996regression}
\citation{abdullah2019}
\citation{guyon2002gene}
\citation{fang2019tightly}
\citation{pouyan2018random}
\citation{ram2017classification}
\citation{wang2012adaboost}
\citation{le2019statistical}
\citation{lu2019autoencoder}
\citation{hotelling1933analysis}
\citation{hinton2006reducing}
\citation{maddison2016concrete}
\citation{kingma2013auto}
\citation{abid2019concrete}
\citation{hinton2006reducing}
\citation{maddison2016concrete}
\citation{jang2016categorical}
\citation{gumbel1954statistical}
\@writefile{toc}{\contentsline {section}{\numberline {2}Materials and Methods}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{2.1}}Coding and Non-coding Gene Expression}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{2.2}}Concrete Relaxation Autoencoder}{2}}
\newlabel{CoRAE}{{{2.2}}{2}}
\citation{kingma2013auto}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  \textbf  {Sample Distribution for 33 cancers along with 75-25 split for training and testing.} }}{3}}
\newlabel{Tab:01}{{1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Architecture of Concrete Relaxation Autoencoder.} Proposed feature selection architecture consists of an encoder and a decoder. The layer after input layer in encoder is called concrete feature selection layer shown in yellow. This layer has $k$ number of node where each node is for each feature to be selected. During the training stage, the $i^{th}$ node $v^{(i)}$ takes the value $x^Tf^{(i)}$. During testing stage, these weights are fixed and the element with the highest value is selected by the corresponding $i^{th}$ hidden node. The architecture of the decoder remains the same during train and test stage.}}{3}}
\newlabel{fig:architecture}{{1}{3}}
\citation{scikit-learn}
\citation{tensorflow2015-whitepaper}
\citation{chollet2015keras}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Concrete relaxation autoencoder}}{4}}
\newlabel{algo}{{1}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{2.3}}Gene Selection, Classification, Reconstruction, and Evaluation}{4}}
\newlabel{method-details}{{{2.3}}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Effect of temperature in reducing search space.} For example, at starting temperature $\tau _0$, the number of input features is 10 and number of feature to be selected $k=3$, at the next temerature $\tau _1$, the number of posibble features reduced to 6, finally at ending $\tau _{stop}$ after some epochs, the search space further reduces to 3 which is equal to the $k$}}{4}}
\newlabel{fig:temp-change}{{2}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Annealing schedules for the CoRAE.} Effect of different annealing schedules on a concrete autoencoder trained on the mRNA dataset with $k$ = 100 selected features. If the temperature is exponentially decayed (the annealing schedule), the feature selected layer (model) converges to informative features.}}{4}}
\newlabel{fig:temp}{{3}{4}}
\citation{maaten2008visualizing}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Classification performance using selected RNA features.} Comparison of CoRAE with other feature selection methods. Throughout the all values of $k$ tested on both mRNA(a) and lncRNA(c) CoRAE have highest classification accuracy. Similarly, it shows lowest reconstruction mean squared error on both mRNA(b) and lncRNA(d)}}{5}}
\newlabel{fig:acc-mse}{{4}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  \textbf  {Classification and reconstruction performances for different number of selected mRNAs and lncRNAs. Here, $k$ is refere to the number of feature to be selected. Perforamnce evaluations for other $k$ are shown in \textit  {supplementary-1} Table 2}}}{6}}
\newlabel{Tab:02}{{2}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Classification accuracy comparison between coding and non-coding genes expression.} Across all the $k$, mRNA expression shows slightly better classification accuracy over lncRNA expression. Here, these features has been seelcted using proposed method only.}}{6}}
\newlabel{fig:acc-mRNA-lncRNA}{{5}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{3.1}}Selected Features Interpretation}{6}}
\newlabel{inter}{{{3.1}}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{6}}
\bibstyle{natbib}
\bibdata{document}
\bibcite{tensorflow2015-whitepaper}{{1}{2015}{{Abadi {\em  et~al.}}}{{Abadi, Agarwal, Barham, Brevdo, Chen, Citro, Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia, Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah, Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan, Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and Zheng}}}
\bibcite{abid2019concrete}{{2}{2019}{{Abid {\em  et~al.}}}{{Abid, Balin, and Zou}}}
\bibcite{chollet2015keras}{{3}{2015}{{Chollet {\em  et~al.}}}{{Chollet {\em  et~al.}}}}
\bibcite{ding2005minimum}{{4}{2005}{{Ding and Peng}}{{Ding and Peng}}}
\bibcite{ding2015identification}{{5}{2015}{{Ding and Li}}{{Ding and Li}}}
\bibcite{fang2019tightly}{{6}{2019}{{Fang}}{{Fang}}}
\bibcite{gumbel1954statistical}{{7}{1954}{{Gumbel}}{{Gumbel}}}
\bibcite{guyon2002gene}{{8}{2002}{{Guyon {\em  et~al.}}}{{Guyon, Weston, Barnhill, and Vapnik}}}
\bibcite{harrow2006gencode}{{9}{2006}{{Harrow {\em  et~al.}}}{{Harrow, Denoeud, Frankish, Reymond, Chen, Chrast, Lagarde, Gilbert, Storey, Swarbreck, {\em  et~al.}}}}
\bibcite{hinton2006reducing}{{10}{2006}{{Hinton and Salakhutdinov}}{{Hinton and Salakhutdinov}}}
\bibcite{hotelling1933analysis}{{11}{1933}{{Hotelling}}{{Hotelling}}}
\bibcite{jang2016categorical}{{12}{2016}{{Jang {\em  et~al.}}}{{Jang, Gu, and Poole}}}
\bibcite{jeong2018feature}{{13}{2018}{{Jeong {\em  et~al.}}}{{Jeong, Kim, Kim, Lee, Kim, and Kang}}}
\bibcite{kabir2012new}{{14}{2012}{{Kabir {\em  et~al.}}}{{Kabir, Shahjahan, and Murase}}}
\bibcite{kingma2013auto}{{15}{2013}{{Kingma and Welling}}{{Kingma and Welling}}}
\bibcite{le2019statistical}{{16}{2019}{{Le {\em  et~al.}}}{{Le, Urbanowicz, Moore, and McKinney}}}
\bibcite{li2015tanric}{{17}{2015}{{Li {\em  et~al.}}}{{Li, Han, Roebuck, Diao, Liu, Yuan, Weinstein, and Liang}}}
\bibcite{liang2018review}{{18}{2018}{{Liang {\em  et~al.}}}{{Liang, Ma, Yang, Wang, and Ma}}}
\bibcite{liu2012feature}{{19}{2012}{{Liu and Motoda}}{{Liu and Motoda}}}
\bibcite{lu2019autoencoder}{{20}{2019}{{Lu {\em  et~al.}}}{{Lu, Gu, Wang, Wang, and Qin}}}
\bibcite{maaten2008visualizing}{{21}{2008}{{Maaten and Hinton}}{{Maaten and Hinton}}}
\bibcite{maddison2016concrete}{{22}{2016}{{Maddison {\em  et~al.}}}{{Maddison, Mnih, and Teh}}}
\bibcite{abdullah2019}{{23}{2019}{{Mamun and Mondal}}{{Mamun and Mondal}}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {Visualization of 33 different cancer types using top-100 CoRAE features.} Here, we show the t-SNE representation of 33 cancer samples using selected features. Each dot represents a cancer sample and each color represents a cancer type. a) t-SNE using top 100 mRNA, b) t-SNE using top 100 lncRNA}}{7}}
\newlabel{fig:tsne}{{6}{7}}
\bibcite{scikit-learn}{{24}{2011}{{Pedregosa {\em  et~al.}}}{{Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, and Duchesnay}}}
\bibcite{pirgazi2019efficient}{{25}{2019}{{Pirgazi {\em  et~al.}}}{{Pirgazi, Alimoradi, Abharian, and Olyaee}}}
\bibcite{pouyan2018random}{{26}{2018}{{Pouyan and Kostka}}{{Pouyan and Kostka}}}
\bibcite{ram2017classification}{{27}{2017}{{Ram {\em  et~al.}}}{{Ram, Najafi, and Shakeri}}}
\bibcite{rau2019exploring}{{28}{2019}{{Rau {\em  et~al.}}}{{Rau, Flister, Rui, and Auer}}}
\bibcite{speed2003statistical}{{29}{2003}{{Speed}}{{Speed}}}
\bibcite{sun2018cross}{{30}{2018}{{Sun {\em  et~al.}}}{{Sun, Lu, and Li}}}
\bibcite{tibshirani1996regression}{{31}{1996}{{Tibshirani}}{{Tibshirani}}}
\bibcite{wang2012adaboost}{{32}{2012}{{Wang}}{{Wang}}}
\bibcite{wu2011feature}{{33}{2011}{{Wu {\em  et~al.}}}{{Wu, Tang, Hor, and Wu}}}
\bibcite{xue2012particle}{{34}{2012}{{Xue {\em  et~al.}}}{{Xue, Zhang, and Browne}}}
\global\@namedef{@lastpage@}{8}
